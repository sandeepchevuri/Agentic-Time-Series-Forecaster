{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024adcf0",
   "metadata": {},
   "source": [
    "# LLM Providers\n",
    "\n",
    "TimeCopilot is designed to be flexible and work with a variety of Large Language Model (LLM) providers. This notebook demonstrates how to configure TimeCopilot to work with different LLM providers, giving you the freedom to choose the model that best fits your needs, budget, and infrastructure requirements.\n",
    "\n",
    "Whether you prefer cloud-based solutions like OpenAI, self-hosted models via Ollama, or other compatible providers, TimeCopilot can adapt to your setup. This flexibility allows you to:\n",
    "- **Control costs** by choosing providers that fit your budget\n",
    "- **Ensure privacy** by using self-hosted models\n",
    "- **Optimize performance** by selecting models that match your use case\n",
    "- **Leverage existing infrastructure** by using providers you already have configured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d5124",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "\n",
    "Let's start by importing the necessary libraries. We'll use `nest_asyncio` to run `TimeCopilot` in Jupyter notebooks, and `pandas` for data manipulation. The `TimeCopilot` class is the main interface we'll use for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494afd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df75198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from timecopilot import TimeCopilot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b73072",
   "metadata": {},
   "source": [
    "# Loading Environment Variables\n",
    "\n",
    "TimeCopilot uses [Pydantic AI](https://ai.pydantic.dev/) to interact with LLMs, which provides excellent support for [many common model providers](https://ai.pydantic.dev/models/overview/) and also supports any endpoint that follows the OpenAI API format.\n",
    "\n",
    "## Why Environment Variables?\n",
    "\n",
    "Most LLM providers require API keys or authentication tokens to access their services. For security reasons, these credentials should never be hardcoded in your scripts. Instead, they should be stored as environment variables, which are loaded at runtime.\n",
    "\n",
    "## Provider-Specific Configuration\n",
    "\n",
    "Configuration requirements vary depending on which provider you're using:\n",
    "- **OpenAI**: Requires `OPENAI_API_KEY` environment variable\n",
    "- **Ollama**: Requires `OLLAMA_BASE_URL` and optionally `OLLAMA_API_KEY`\n",
    "- **Other providers**: Each has its own specific environment variables (see Pydantic's documentation for details)\n",
    "\n",
    "Below are three common methods for setting environment variables. Choose the one that best fits your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfa336",
   "metadata": {},
   "source": [
    "### Method 1: Directly Set in Python\n",
    "\n",
    "This method is useful for quick testing or when you need to set variables programmatically. **Note**: This is less secure for production use, as the key is visible in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d199616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-new-secret-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79021447",
   "metadata": {},
   "source": [
    "### Method 2: Set in Your Shell\n",
    "\n",
    "This method sets the environment variable in your terminal session. The variable will be available to all processes started from that terminal, but will be lost when you close the terminal.\n",
    "\n",
    "**Important**: This needs to be done before starting your Python process (before running `jupyter notebook`, `python script.py`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0f3e0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# bash\n",
    "export OPENAI_API_KEY=\"your-new-secret-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e844b",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# powershell\n",
    "setx OPENAI_API_KEY \"your-new-secret-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826ba3e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Method 3: Store in a .env File (Recommended)\n",
    "\n",
    "This is the most convenient and secure method for development. You create a `.env` file in your project directory (make sure to add it to `.gitignore` to avoid committing secrets), and load it using the `python-dotenv` package.\n",
    "\n",
    "**Benefits**:\n",
    "- Keeps credentials out of your code\n",
    "- Easy to manage multiple environment variables\n",
    "- Works consistently across different operating systems\n",
    "- Can be easily excluded from version control"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31ac2485",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# example .env file\n",
    "OPENAI_API_KEY=\"your-new-secret-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a17c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4f0d7",
   "metadata": {},
   "source": [
    "# Supported LLM Providers\n",
    "\n",
    "TimeCopilot supports all LLM providers that are compatible with Pydantic AI. For a comprehensive list of officially supported providers, visit [Pydantic's model providers overview](https://ai.pydantic.dev/models/overview/).\n",
    "\n",
    "Each provider has its own setup requirements and configuration options, which are documented in Pydantic's documentation. In this notebook, we'll demonstrate a few common providers:\n",
    "\n",
    "1. **OpenAI** - Cloud-based, widely used, excellent performance\n",
    "2. **Ollama** - Self-hosted, privacy-focused, runs models locally\n",
    "3. **OpenAI-compatible endpoints** - Any service that follows the OpenAI API format\n",
    "\n",
    "Let's explore each of these options with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e148c",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "OpenAI provides some of the most powerful and widely-used language models, including GPT-5.1, GPT-4, GPT-3.5, and GPT-4o. These models offer excellent performance for TimeCopilot's forecasting tasks.\n",
    "\n",
    "### When to Use OpenAI\n",
    "- You need high-quality model outputs\n",
    "- You're comfortable with cloud-based solutions\n",
    "- You have an OpenAI API account and budget\n",
    "- You want reliable, production-ready models\n",
    "\n",
    "### Set the Environment Variable\n",
    "\n",
    "`OPENAI_API_KEY` is the required environment variable for OpenAI. You can get your API key from the [OpenAI Platform](https://platform.openai.com/api-keys).\n",
    "\n",
    "Load it with your preferred method (the three methods are described above). Here's an example using the `.env` file approach:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00ea5516",
   "metadata": {},
   "source": [
    "# example .env file\n",
    "OPENAI_API_KEY=\"your-new-secret-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .env file with python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab1518",
   "metadata": {},
   "source": [
    "### Initialize the Forecasting Agent\n",
    "\n",
    "Now that your API key is configured, you can initialize TimeCopilot with an OpenAI model. The `llm` parameter accepts a string in the format `\"provider:model-name\"`. For OpenAI, you can use models like:\n",
    "- `\"openai:gpt-4o\"` - Latest GPT-4 optimized model (recommended)\n",
    "- `\"openai:gpt-4\"` - GPT-4 model\n",
    "- `\"openai:gpt-3.5-turbo\"` - Faster and more cost-effective option\n",
    "\n",
    "The `retries` parameter specifies how many times TimeCopilot should retry failed API calls, which helps with reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba3ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TimeCopilot(\n",
    "    llm=\"openai:gpt-4o\",\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3872de9",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Ollama is a powerful tool for running large language models locally on your own machine. This gives you complete control over your data and eliminates API costs, making it ideal for:\n",
    "- **Privacy-sensitive applications** - Your data never leaves your machine\n",
    "- **Cost optimization** - No per-request API fees\n",
    "- **Offline usage** - Works without internet connectivity\n",
    "- **Custom models** - Run specialized or fine-tuned models\n",
    "\n",
    "### When to Use Ollama\n",
    "- You have sufficient local compute resources (GPU recommended)\n",
    "- Privacy is a primary concern\n",
    "- You want to avoid API costs\n",
    "- You need to work with custom or specialized models\n",
    "\n",
    "### Set the Environment Variables\n",
    "\n",
    "Ollama requires the `OLLAMA_BASE_URL` environment variable, and optionally `OLLAMA_API_KEY` depending on your configuration:\n",
    "\n",
    "- **Local installation**: If you're running Ollama locally, you typically don't need an API key. The default URL is `http://localhost:11434/v1`\n",
    "- **Ollama Cloud**: If you're using Ollama's cloud service, you'll need both variables with `OLLAMA_BASE_URL` set to `https://ollama.com/v1`\n",
    "\n",
    "To get started with Ollama locally:\n",
    "1. Install Ollama from [ollama.ai](https://ollama.ai)\n",
    "2. Pull a model: `ollama pull gpt-oss:20b` (or any other model)\n",
    "3. Set the environment variables as shown below\n",
    "\n",
    "Load your environment variables with your preferred method (the three methods are described above): "
   ]
  },
  {
   "cell_type": "raw",
   "id": "44efd7a4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# example .env file\n",
    "\n",
    "OLLAMA_BASE_URL=\"http://localhost:11434/v1\"\n",
    "OLLAMA_API_KEY=\"your-new-secret-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .env file with python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9bebb",
   "metadata": {},
   "source": [
    "### Initialize the Forecasting Agent\n",
    "\n",
    "There are two ways to initialize TimeCopilot with Ollama. The first is simpler and uses a string format, while the second gives you more control over the configuration.\n",
    "\n",
    "#### Approach 1: Simple String Format (Recommended)\n",
    "\n",
    "This is the easiest way to use Ollama. Just specify the provider and model name in the format `\"ollama:model-name\"`. TimeCopilot will automatically use the `OLLAMA_BASE_URL` from your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64993451",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TimeCopilot(\n",
    "    llm='ollama:gpt-oss:20b',\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e86c8b",
   "metadata": {},
   "source": [
    "#### Approach 2: Direct Provider Initialization\n",
    "\n",
    "If you need more control over the configuration (for example, to use a different base URL than what's in your environment variables), you can initialize the model and provider directly. This approach is also useful when you want to programmatically set the base URL or use multiple Ollama instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider\n",
    "\n",
    "llm = OpenAIChatModel(\n",
    "    model_name=\"gpt-oss:20b\",\n",
    "    provider=OllamaProvider(base_url=\"http://localhost:11434/v1\"),\n",
    ")\n",
    "\n",
    "tc = TimeCopilot(\n",
    "    llm=llm,\n",
    "    retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9dd13",
   "metadata": {},
   "source": [
    "## OpenAI-Compatible Endpoints\n",
    "\n",
    "Many LLM providers and self-hosted solutions offer APIs that are compatible with OpenAI's API format. This includes:\n",
    "- **LocalAI** - Self-hosted OpenAI-compatible API\n",
    "- **vLLM** - High-performance inference server\n",
    "- **Text Generation Inference (TGI)** - Hugging Face's inference server\n",
    "- **Custom deployments** - Any service that follows OpenAI's API specification\n",
    "\n",
    "### When to Use OpenAI-Compatible Endpoints\n",
    "- You're running your own inference server\n",
    "- You want to use models not available through standard providers\n",
    "- You need fine-grained control over the API endpoint\n",
    "- You're using a provider that supports OpenAI's API format but isn't directly supported by Pydantic\n",
    "\n",
    "### How It Works\n",
    "\n",
    "These providers use OpenAI's API format, so we can use Pydantic's `OpenAIChatModel` with a custom `OpenAIProvider` that points to your endpoint. This gives you the flexibility to use any OpenAI-compatible service with TimeCopilot.\n",
    "\n",
    "### Example: Using a Local OpenAI-Compatible Endpoint\n",
    "\n",
    "The following example shows how to configure TimeCopilot to use a local inference server running on `http://127.0.0.1:1234/v1`. Adjust the `LLM_API_BASE_URL`, `MODEL_NAME`, and `API_KEY` to match your setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "LLM_API_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "MODEL_NAME = \"gpt-oss-20b\"\n",
    "API_KEY = \"api-key\"\n",
    "\n",
    "model = OpenAIChatModel(\n",
    "    MODEL_NAME,\n",
    "    provider=OpenAIProvider(\n",
    "        base_url=LLM_API_BASE_URL,\n",
    "        api_key=API_KEY,\n",
    "    ),\n",
    ")\n",
    "\n",
    "tc = TimeCopilot(\n",
    "    llm=model,\n",
    "    retries=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93349f86",
   "metadata": {},
   "source": [
    "# Using TimeCopilot with Your Chosen Provider\n",
    "\n",
    "Once you've configured your LLM provider and initialized TimeCopilot, you can use it exactly as you would with any other provider. The interface remains consistent regardless of which provider you choose, making it easy to switch between providers or test different models.\n",
    "\n",
    "Let's demonstrate this with a practical example using the classic Air Passengers dataset. This will show how TimeCopilot:\n",
    "1. Analyzes your time series data\n",
    "2. Selects appropriate forecasting models\n",
    "3. Generates forecasts\n",
    "4. Answers natural language questions about the forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7155753",
   "metadata": {},
   "source": [
    "First, let's load a sample time series dataset. The Air Passengers dataset contains monthly totals of international airline passengers from 1949 to 1960, which is a classic example for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21a5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       unique_id          ds    y\n",
      "0  AirPassengers  1949-01-01  112\n",
      "1  AirPassengers  1949-02-01  118\n",
      "2  AirPassengers  1949-03-01  132\n",
      "3  AirPassengers  1949-04-01  129\n",
      "4  AirPassengers  1949-05-01  121\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://timecopilot.s3.amazonaws.com/public/data/air_passengers.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e95bf",
   "metadata": {},
   "source": [
    "## Generate Forecasts\n",
    "\n",
    "Now let's use TimeCopilot to generate forecasts. The `forecast` method will:\n",
    "1. Analyze the time series characteristics\n",
    "2. Select appropriate forecasting models based on the data\n",
    "3. Train and evaluate multiple models\n",
    "4. Return the best forecast along with detailed analysis\n",
    "\n",
    "The `freq=\"MS\"` parameter specifies that the data has monthly frequency (Month Start).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.48it/s]\n",
      "1it [00:00, 25.92it/s]\n",
      "1it [00:00, 251.31it/s]\n",
      "0it [00:00, ?it/s]15:51:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "1it [00:03,  3.55s/it]\n",
      "15:51:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "0it [00:00, ?it/s]15:51:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "1it [00:02,  2.97s/it]15:51:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "2it [00:05,  2.97s/it]15:51:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "3it [00:09,  3.03s/it]15:51:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "4it [00:12,  3.05s/it]15:51:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "5it [00:15,  3.06s/it]15:51:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "6it [00:18,  3.07s/it]15:51:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "7it [00:21,  3.12s/it]15:51:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "8it [00:24,  3.13s/it]15:51:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "9it [00:27,  3.09s/it]15:51:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "10it [00:30,  3.06s/it]15:51:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11it [00:33,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "result = tc.forecast(df=df, freq=\"MS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17811d24",
   "metadata": {},
   "source": [
    "## View the Analysis\n",
    "\n",
    "TimeCopilot provides detailed analysis of your time series. The `tsfeatures_analysis` contains insights about:\n",
    "- **Stability**: How stable the series is over time\n",
    "- **Stationarity**: Whether the series has trends or is stationary\n",
    "- **Seasonality**: The strength of seasonal patterns\n",
    "- **Series characteristics**: Length, complexity, and other features\n",
    "\n",
    "This analysis helps TimeCopilot select the most appropriate forecasting models for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13902a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time series analysis reveals several key attributes which will guide our model selection:\n",
      "\n",
      "1. **Stability (0.933)**: A higher value indicates a relatively stable time series, which is consistent with the seasonal pattern typical in a dataset like AirPassengers.\n",
      "2. **Unit Root Tests**: The PP test (-6.57) suggests the series is stationary, whereas the KPSS test (2.74) indicates non-stationarity. This mixed signal is somewhat expected in real-world data and suggests models that handle both trends and seasonality might perform well.\n",
      "3. **Trend and Seasonality**: The series includes a strong trend (0.997) and pronounced seasonal strength (0.982). This points to models that can effectively capture both components, such as Prophet, which explicitly models both trend and seasonality.\n",
      "4. **Series Length (144)**: A reasonably long series enables the usage of sophisticated models that require more data to perform effectively.\n"
     ]
    }
   ],
   "source": [
    "print(result.output.tsfeatures_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b6c06",
   "metadata": {},
   "source": [
    "## View the Forecasts\n",
    "\n",
    "The `fcst_df` contains the actual forecast values. Each row represents a future time point with the predicted value from the selected model (in this case, Prophet was chosen based on the time series characteristics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        unique_id         ds     Prophet\n",
      "0   AirPassengers 1961-01-01  466.560401\n",
      "1   AirPassengers 1961-02-01  461.042082\n",
      "2   AirPassengers 1961-03-01  493.413542\n",
      "3   AirPassengers 1961-04-01  492.113653\n",
      "4   AirPassengers 1961-05-01  496.445709\n",
      "5   AirPassengers 1961-06-01  537.592041\n",
      "6   AirPassengers 1961-07-01  577.166093\n",
      "7   AirPassengers 1961-08-01  577.599117\n",
      "8   AirPassengers 1961-09-01  529.038266\n",
      "9   AirPassengers 1961-10-01  493.889181\n",
      "10  AirPassengers 1961-11-01  460.030234\n",
      "11  AirPassengers 1961-12-01  489.392785\n",
      "12  AirPassengers 1962-01-01  502.415939\n",
      "13  AirPassengers 1962-02-01  496.321423\n",
      "14  AirPassengers 1962-03-01  531.969966\n",
      "15  AirPassengers 1962-04-01  528.065107\n",
      "16  AirPassengers 1962-05-01  534.174659\n",
      "17  AirPassengers 1962-06-01  573.615281\n",
      "18  AirPassengers 1962-07-01  614.245102\n",
      "19  AirPassengers 1962-08-01  614.206790\n",
      "20  AirPassengers 1962-09-01  566.306418\n",
      "21  AirPassengers 1962-10-01  530.606803\n",
      "22  AirPassengers 1962-11-01  497.766797\n",
      "23  AirPassengers 1962-12-01  527.289739\n"
     ]
    }
   ],
   "source": [
    "print(result.fcst_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3dd4a",
   "metadata": {},
   "source": [
    "## Ask Questions About Your Forecasts\n",
    "\n",
    "One of TimeCopilot's powerful features is the ability to answer natural language questions about your forecasts. The `query` method uses the LLM to understand your question and provide insights based on the forecast data.\n",
    "\n",
    "This demonstrates how the LLM provider you chose is actively being used to understand and respond to your queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f591285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate the total number of passengers forecasted for the next months using the Prophet model, I will sum up the forecasted values. Here are the forecasted passenger numbers for the next several months:\n",
      "\n",
      "1. 466.56\n",
      "2. 461.04\n",
      "3. 493.41\n",
      "4. 492.11\n",
      "5. 496.45\n",
      "6. 537.59\n",
      "7. 577.17\n",
      "8. 577.60\n",
      "9. 529.04\n",
      "10. 493.89\n",
      "11. 460.03\n",
      "12. 489.39\n",
      "\n",
      "Adding these values gives us a total of approximately \\( 6,074.98 \\) passengers forecasted over the next 12 months.\n",
      "\n",
      "This forecast is based on the Prophet model, which has a MASE score of 1.09, indicating its estimation accuracy is relatively good compared to other models.\n"
     ]
    }
   ],
   "source": [
    "query_result = tc.query(\"how many passengers will be in total in the next months?\")\n",
    "print(query_result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b899bc",
   "metadata": {},
   "source": [
    "# Model Compatibility Requirements\n",
    "\n",
    "## Tool Use is Required\n",
    "\n",
    "TimeCopilot requires models that support **tool use** (also known as function calling). This is a critical feature that allows the LLM to:\n",
    "- Call forecasting functions and methods\n",
    "- Execute code to analyze time series\n",
    "- Interact with statistical models\n",
    "- Generate structured outputs\n",
    "\n",
    "### What This Means\n",
    "\n",
    "Not every LLM model supports tool use. When choosing a model provider, ensure that:\n",
    "1. The model supports function calling/tool use\n",
    "2. The provider's API exposes this capability\n",
    "3. Pydantic AI can access this feature through the provider\n",
    "\n",
    "### Compatible Models\n",
    "\n",
    "Most modern, capable models support tool use, including:\n",
    "- **OpenAI models**: GPT-4, GPT-4o, GPT-3.5-turbo (with function calling)\n",
    "- **Ollama models**: Many models support tool use (check model documentation)\n",
    "- **OpenAI-compatible endpoints**: If the endpoint supports function calling\n",
    "\n",
    "### Testing Compatibility\n",
    "\n",
    "If you're unsure whether a model supports tool use, try initializing TimeCopilot with it. If the model doesn't support tool use, you'll receive an error when attempting to use TimeCopilot's features.\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "If you encounter compatibility issues:\n",
    "1. Check the model provider's documentation for tool use/function calling support\n",
    "2. Verify that Pydantic AI supports your provider\n",
    "3. Consider using a known-compatible model like GPT-4o or GPT-4\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook has demonstrated how TimeCopilot can work with different LLM providers, giving you flexibility in choosing the right model for your needs. Whether you prefer cloud-based solutions, self-hosted models, or custom endpoints, TimeCopilot provides a consistent interface that works across providers.\n",
    "\n",
    "**Key Takeaways**:\n",
    "- TimeCopilot supports any LLM provider compatible with Pydantic AI\n",
    "- Configuration is done through environment variables\n",
    "- The interface remains consistent regardless of provider\n",
    "- Models must support tool use/function calling\n",
    "- You can easily switch between providers by changing the `llm` parameter\n",
    "\n",
    "For more information, visit:\n",
    "- [Pydantic AI Documentation](https://ai.pydantic.dev/)\n",
    "- [TimeCopilot Documentation](https://timecopilot.dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c4234",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timecopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
